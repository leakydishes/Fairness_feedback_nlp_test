{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPsvt7irUW7ViKY8aG2/AX6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leakydishes/Fairness_feedback_nlp_test/blob/main/Fairness_feedback_test_sep1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "D9pZdn0536bB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8feedf-89e5-4edd-ce16-1d96475f64e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug 31 15:32:37 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Set Up Google Colab"
      ],
      "metadata": {
        "id": "atQmO1X5vHTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "jx4Ie-Ocq0wm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68e9438-ef1e-48a2-c8b5-5edfa97e0a05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dIXUlvUUmd3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6337b3e6-3631-471b-c300-236d84377afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairness-feedback-nlp'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 53 (delta 14), reused 42 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (53/53), 11.56 MiB | 13.74 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "/content/fairness-feedback-nlp\n"
          ]
        }
      ],
      "source": [
        "# Clone Project\n",
        "!git clone https://github.com/eth-sri/fairness-feedback-nlp.git\n",
        "%cd fairness-feedback-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download train.csv and test.csv\n",
        "#!cd fairness-feedback-nlp/Code/Datasets/Kaggle_Toxicity"
      ],
      "metadata": {
        "id": "NPyPOtCeno1x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P Code/Datasets/Kaggle_Toxicity/ \"https://raw.githubusercontent.com/eth-sri/fairness-feedback-nlp/master/Data/train.csv\"\n",
        "!wget -P Code/Datasets/Kaggle_Toxicity/ \"https://raw.githubusercontent.com/eth-sri/fairness-feedback-nlp/master/Data/test.csv\""
      ],
      "metadata": {
        "id": "4bIoFpfOBxKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7920a73b-a467-4b69-85d6-e1cd792ec8cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-31 15:32:39--  https://raw.githubusercontent.com/eth-sri/fairness-feedback-nlp/master/Data/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38690714 (37M) [text/plain]\n",
            "Saving to: ‘Code/Datasets/Kaggle_Toxicity/train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  36.90M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-08-31 15:32:41 (322 MB/s) - ‘Code/Datasets/Kaggle_Toxicity/train.csv’ saved [38690714/38690714]\n",
            "\n",
            "--2023-08-31 15:32:41--  https://raw.githubusercontent.com/eth-sri/fairness-feedback-nlp/master/Data/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11490615 (11M) [text/plain]\n",
            "Saving to: ‘Code/Datasets/Kaggle_Toxicity/test.csv’\n",
            "\n",
            "test.csv            100%[===================>]  10.96M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-08-31 15:32:42 (240 MB/s) - ‘Code/Datasets/Kaggle_Toxicity/test.csv’ saved [11490615/11490615]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Python"
      ],
      "metadata": {
        "id": "zRxp2vZG2wrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/fairness-feedback-nlp/Code\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install python3.8\n",
        "!python3.8 -m ensurepip --upgrade\n",
        "\n",
        "!ls /usr/bin/python*\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "\n",
        "#permanently install the specific version to the google colab\n",
        "!sudo apt install python3-pip\n",
        "\n",
        "\n",
        "#Install correct packages\n",
        "!sudo apt-get install python3.8-distutils"
      ],
      "metadata": {
        "id": "GOj_K6rzv6sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001e0392-0c23-4a6e-80e0-c2d6e0625b73"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/fairness-feedback-nlp/Code'\n",
            "/content/fairness-feedback-nlp\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [913 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [989 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [990 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,176 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,251 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,015 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,185 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,117 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [494 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [21.8 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [37.3 kB]\n",
            "Fetched 10.6 MB in 1s (9,979 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support\n",
            "  python3.8-minimal\n",
            "Suggested packages:\n",
            "  python3.8-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support python3.8\n",
            "  python3.8-minimal\n",
            "0 upgraded, 6 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 5,098 kB of archives.\n",
            "After this operation, 18.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.18-1+jammy1 [794 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.18-1+jammy1 [2,024 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.18-1+jammy1 [1,815 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.18-1+jammy1 [438 kB]\n",
            "Fetched 5,098 kB in 0s (10.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../1-python3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.18-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../4-libpython3.8-stdlib_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../5-python3.8_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.18-1+jammy1) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n",
            "Setting up python3.8-minimal (3.8.18-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n",
            "Setting up python3.8 (3.8.18-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "/usr/bin/python3.8: No module named ensurepip\n",
            "/usr/bin/python3     /usr/bin/python3.10-config  /usr/bin/python3-config\n",
            "/usr/bin/python3.10  /usr/bin/python3.8\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,965 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.3 [1,305 kB]\n",
            "Fetched 1,677 kB in 0s (15.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 121482 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.3_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.3) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3.8-lib2to3\n",
            "The following NEW packages will be installed:\n",
            "  python3.8-distutils python3.8-lib2to3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 319 kB of archives.\n",
            "After this operation, 1,237 kB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.18-1+jammy1 [126 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.18-1+jammy1 [193 kB]\n",
            "Fetched 319 kB in 0s (1,619 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3.8-lib2to3.\n",
            "(Reading database ... 122344 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.8-lib2to3_3.8.18-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-lib2to3 (3.8.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-distutils.\n",
            "Preparing to unpack .../python3.8-distutils_3.8.18-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-distutils (3.8.18-1+jammy1) ...\n",
            "Setting up python3.8-lib2to3 (3.8.18-1+jammy1) ...\n",
            "Setting up python3.8-distutils (3.8.18-1+jammy1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "C8ym4uGSwsB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5a6fe8-e380-4757-b275-5070005c9866"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Setting Up Environment and Dependencies"
      ],
      "metadata": {
        "id": "To_oZ8lovO5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LgObeqOkvW65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!chmod +x Generation.sh\n",
        "!chmod +x Generation_Quick.sh"
      ],
      "metadata": {
        "id": "vAE0Xy2zx1y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94d57a6-ec08-4894-e5c7-58cbd6f8370e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access 'Generation_Quick.sh': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "QOCwxABnqXZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d8a85c6-33fa-4709-cabd-777e31c80186"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.17\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2023.8.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.3/774.3 KB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.15.1\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 KB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.12.3-py3-none-any.whl (11 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1\n",
            "  Downloading safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.6/736.6 KB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, safetensors, urllib3, typing-extensions, tqdm, regex, pyyaml, packaging, numpy, idna, fsspec, charset-normalizer, certifi, requests, filelock, huggingface-hub, transformers\n",
            "Successfully installed certifi-2023.7.22 charset-normalizer-3.2.0 filelock-3.12.3 fsspec-2023.6.0 huggingface-hub-0.16.4 idna-3.4 numpy-1.24.4 packaging-23.1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 safetensors-0.3.3 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.32.1 typing-extensions-4.7.1 urllib3-2.0.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pwd\n",
        "!pip install torch torchvision -U\n",
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "# import os\n",
        "# os.listdir('.')"
      ],
      "metadata": {
        "id": "FJIiIFyCrJgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52313201-5e60-461b-ee72-7de8046c6091"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code  Data  LICENSE  overview.png  README.md\n",
            "/content/fairness-feedback-nlp\n",
            "Collecting torch\n",
            "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.15.2-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.7.1)\n",
            "Collecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.12.3)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
            "Collecting lit\n",
            "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cmake\n",
            "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.24.4)\n",
            "Collecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.31.0)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=016f543750081db8514afd019733ccd73c33254ac50338dfd9f35fa56aca0109\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ab/f1/0102fea49a41c753f0e79a1a4012417d5d7ef0f93224694472\n",
            "Successfully built lit\n",
            "Installing collected packages: mpmath, lit, cmake, sympy, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, MarkupSafe, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, triton, torch, torchvision\n",
            "Successfully installed MarkupSafe-2.1.3 cmake-3.27.2 jinja2-3.1.2 lit-16.0.6 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pillow-10.0.0 sympy-1.12 torch-2.0.1 torchvision-0.15.2 triton-2.0.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/fairness-feedback-nlp\n",
        "# !pwd\n",
        "# !fairness-feedback-nlp/bin/activate\n",
        "!chmod 755 /content/fairness-feedback-nlp/Code/Generation_Quick.sh\n",
        "%cd /content/fairness-feedback-nlp/Code\n",
        "%ls"
      ],
      "metadata": {
        "id": "U8RA-n2or5ra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da05167-e1da-4a81-89df-64b74994e099"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fairness-feedback-nlp/Code\n",
            "Bart_Label.py         key.txt                 \u001b[0m\u001b[01;34mTransfer\u001b[0m/\n",
            "CLP_Robustness.py     requirements.txt        Transfer_GPT_Davinci_Edit.py\n",
            "\u001b[01;34mDatasets\u001b[0m/             \u001b[01;34mResults\u001b[0m/                Transfer_GPT_Davinci.py\n",
            "Generation_GPT.sh     Roberta.py              Transfer_GPT_Edit.py\n",
            "\u001b[01;32mGeneration_Quick.sh\u001b[0m*  Robustness_Transfer.py  Transfer.py\n",
            "\u001b[01;34mgenerations\u001b[0m/          Tables_GPT.sh           Transfer_WR_50.py\n",
            "Generation.sh         Tables.sh               Transfer_WR_Full.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run\n"
      ],
      "metadata": {
        "id": "fRogS5ueF2rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.22.3\n",
        "!pip install torch==1.11.0\n",
        "!pip install tqdm==4.64.0\n",
        "!pip install transformers==4.18.0\n",
        "!pip install pandas==1.4.2\n"
      ],
      "metadata": {
        "id": "7oE83G5JF86l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b871de21-e747-4a3b-80fe-c0b011206793"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.22.3\n",
            "  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "Successfully installed numpy-1.22.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0) (4.7.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.2 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tqdm==4.64.0\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "Successfully installed tqdm-4.64.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (3.12.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (2023.8.8)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (0.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (6.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (1.22.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.7.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (2023.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (2.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (3.2.0)\n",
            "Collecting click\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 KB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.18.0) (1.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=c2d5f446658cc68fb443aa2a839412d361221922bdecba1c532f6c95ce7c7172\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, joblib, click, sacremoses, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.32.1\n",
            "    Uninstalling transformers-4.32.1:\n",
            "      Successfully uninstalled transformers-4.32.1\n",
            "Successfully installed click-8.1.7 joblib-1.3.2 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pandas==1.4.2\n",
            "  Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 KB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.2) (1.22.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas==1.4.2) (1.16.0)\n",
            "Installing collected packages: pytz, python-dateutil, pandas\n",
            "Successfully installed pandas-1.4.2 python-dateutil-2.8.2 pytz-2023.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze | grep pandas"
      ],
      "metadata": {
        "id": "1zbyPdkALppm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa98f46-df2b-44c9-f500-dbb41841db1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pandas==1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy==1.6.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxc3zj9JPjV0",
        "outputId": "1ae1a8ae-99c8-4676-c6ff-d09cc6109f7e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.6.3\n",
            "  Downloading scipy-1.6.3-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy==1.6.3) (1.22.3)\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.6.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate modified comments\n",
        "!./Generation_Quick.sh\n"
      ],
      "metadata": {
        "id": "JK8eGGmsnquF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce93594-aba1-4f72-95da-b526c0260af5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 878k/878k [00:00<00:00, 2.76MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 2.79MB/s]\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
            "/content/fairness-feedback-nlp/Code/Transfer/Datasets/Datasets.py:93: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  self.pos_weights = (1-(self.y>0.5).mean(0))/(self.y>0.5).mean(0)\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "8464\n",
            "846\n",
            "epoch:  0\n",
            "Batch_loss $nan: 100% 477/477 [00:41<00:00, 11.38it/s]\n",
            "epoch:  1\n",
            "Batch_loss $nan: 100% 477/477 [00:40<00:00, 11.64it/s]\n",
            "epoch:  2\n",
            "Batch_loss $nan: 100% 477/477 [00:41<00:00, 11.63it/s]\n",
            "Testing on train\n",
            "100% 477/477 [00:11<00:00, 42.24it/s]\n",
            "train male\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train female\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train transgender\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train other_gender\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train heterosexual\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train homosexual_gay_or_lesbian\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train bisexual\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train other_sexual_orientation\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train christian\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train jewish\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train muslim\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train hindu\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train buddhist\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train atheist\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train other_religion\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train black\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train white\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train asian\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train latino\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train other_race_or_ethnicity\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train physical_disability\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train intellectual_or_learning_disability\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train psychiatric_or_mental_illness\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "train other_disability\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "Testing on test\n",
            "100% 53/53 [00:01<00:00, 42.26it/s]\n",
            "test male\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test female\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test transgender\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test other_gender\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test heterosexual\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test homosexual_gay_or_lesbian\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test bisexual\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test other_sexual_orientation\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test christian\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test jewish\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test muslim\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test hindu\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test buddhist\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test atheist\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test other_religion\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test black\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test white\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test asian\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test latino\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test other_race_or_ethnicity\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test physical_disability\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test intellectual_or_learning_disability\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test psychiatric_or_mental_illness\n",
            "tpr: tensor(0., device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "test other_disability\n",
            "tpr: tensor(nan, device='cuda:0')\n",
            "tnr: tensor(1., device='cuda:0')\n",
            "loss: tensor(nan, device='cuda:0')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Datasets.py\", line 58, in __init__\n",
            "    data_train = pd.read_csv(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\", line 680, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\", line 581, in _read\n",
            "    return parser.read(nrows)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\", line 1254, in read\n",
            "    index, columns, col_dict = self._engine.read(nrows)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\", line 225, in read\n",
            "    chunks = self._reader.read_low_memory(nrows)\n",
            "  File \"pandas/_libs/parsers.pyx\", line 805, in pandas._libs.parsers.TextReader.read_low_memory\n",
            "  File \"pandas/_libs/parsers.pyx\", line 861, in pandas._libs.parsers.TextReader._read_rows\n",
            "  File \"pandas/_libs/parsers.pyx\", line 847, in pandas._libs.parsers.TextReader._tokenize_rows\n",
            "  File \"pandas/_libs/parsers.pyx\", line 1960, in pandas._libs.parsers.raise_parser_error\n",
            "pandas.errors.ParserError: Error tokenizing data. C error: EOF inside string starting at row 135747\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'comment_text'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"Bart_Label.py\", line 8, in <module>\n",
            "    train_generator(batch_size=3,max_length=64, lr=1e-5, epochs=3 ,mode=\"Bart\",attention_layer=10,\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Training.py\", line 108, in train_generator\n",
            "    dataset = Kaggle_Toxicity(label_subset=[0,1,2,4,5,6,8,9,10,11,12,13,15,16,17,18,22],max_length=max_length)\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Datasets.py\", line 63, in __init__\n",
            "    data_train[\"comment_text\"] = data_train[\"comment_text\"].str.replace(\"\\n\", \" \").str.replace(\"\\xa0\", \" \")\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
            "    indexer = self.columns.get_loc(key)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'comment_text'\n",
            "Transfering from: 0  to: 0\n",
            "Transfering from: 0  to: 1\n",
            "Traceback (most recent call last):\n",
            "  File \"Transfer.py\", line 14, in <module>\n",
            "    generate(model_type=\"Bart\",removal_mode=\"drop_thresh\",q=\"mean\",remove_label=remove_label,target_label=target_label,n_beams=5,max_length=64,\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Attribute_Transfer.py\", line 32, in generate\n",
            "    generator.load_state_dict(torch.load(generator_path))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 699, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 231, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 212, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'generations/Bart_test_label_attention_mean_640'\n",
            "Transfering from: 0  to: 0\n",
            "Transfering from: 0  to: 1\n",
            "Traceback (most recent call last):\n",
            "  File \"Transfer_WR_Full.py\", line 14, in <module>\n",
            "    generate(model_type=None,removal_mode=\"drop_thresh\",q=\"mean\",remove_label=remove_label,target_label=target_label,n_beams=5,max_length=64,\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Attribute_Transfer.py\", line 37, in generate\n",
            "    dataset = Kaggle_Toxicity(max_length=max_length)\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Datasets.py\", line 78, in __init__\n",
            "    dataset = dataset[token_num <= max_length]\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 3496, in __getitem__\n",
            "    return self._getitem_bool_array(key)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 3543, in _getitem_bool_array\n",
            "    raise ValueError(\n",
            "ValueError: Item wrong length 81572 instead of 140285.\n",
            "Transfering from: 0  to: 0\n",
            "Transfering from: 0  to: 1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Datasets.py\", line 58, in __init__\n",
            "    data_train = pd.read_csv(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\", line 680, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\", line 581, in _read\n",
            "    return parser.read(nrows)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\", line 1254, in read\n",
            "    index, columns, col_dict = self._engine.read(nrows)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\", line 225, in read\n",
            "    chunks = self._reader.read_low_memory(nrows)\n",
            "  File \"pandas/_libs/parsers.pyx\", line 805, in pandas._libs.parsers.TextReader.read_low_memory\n",
            "  File \"pandas/_libs/parsers.pyx\", line 861, in pandas._libs.parsers.TextReader._read_rows\n",
            "  File \"pandas/_libs/parsers.pyx\", line 847, in pandas._libs.parsers.TextReader._tokenize_rows\n",
            "  File \"pandas/_libs/parsers.pyx\", line 1960, in pandas._libs.parsers.raise_parser_error\n",
            "pandas.errors.ParserError: Error tokenizing data. C error: EOF inside string starting at row 142511\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'comment_text'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"Transfer_WR_50.py\", line 15, in <module>\n",
            "    generate(model_type=None,removal_mode=\"drop_thresh\",q=\"mean\",remove_label=remove_label,target_label=target_label,n_beams=5,max_length=64,\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Attribute_Transfer.py\", line 37, in generate\n",
            "    dataset = Kaggle_Toxicity(max_length=max_length)\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Datasets/Datasets.py\", line 63, in __init__\n",
            "    data_train[\"comment_text\"] = data_train[\"comment_text\"].str.replace(\"\\n\", \" \").str.replace(\"\\xa0\", \" \")\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
            "    indexer = self.columns.get_loc(key)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'comment_text'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x Tables.sh"
      ],
      "metadata": {
        "id": "kFJYZkkr26EZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Tables.sh"
      ],
      "metadata": {
        "id": "RAzfKfegHzyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c02c21-f4d1-45e8-8827-5bf119a44b49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n",
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n",
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n",
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n",
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n",
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n",
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n",
            "Traceback (most recent call last):\n",
            "  File \"Robustness_Transfer.py\", line 2, in <module>\n",
            "    from Transfer.Training.Active import human_loop,init_human_loop,load_active_state,save_active_state,eval_active,conf_interval_dict,eval_robustness,train_regularized_filtered\n",
            "  File \"/content/fairness-feedback-nlp/Code/Transfer/Training/Active.py\", line 14, in <module>\n",
            "    import scipy.stats\n",
            "ModuleNotFoundError: No module named 'scipy'\n"
          ]
        }
      ]
    }
  ]
}
